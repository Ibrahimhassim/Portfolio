# -*- coding: utf-8 -*-
"""Untitled17.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JVMnWVDKxS5yDnur5668t71skYhirmJh
"""

# Import necessary libraries
import os
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Define Paths for Saving Outputs
output_dir = 'cifar10_pipeline_results'  # Directory to store model outputs
os.makedirs(output_dir, exist_ok=True)  # Create directory if it doesn't exist

# Step 2: Load CIFAR-10 dataset
def load_data():
    # Load CIFAR-10 dataset and normalize pixel values
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()
    x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize images to the range [0,1]
    y_test = y_test.flatten()  # Flatten label arrays
    y_train = y_train.flatten()
    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']  # Class names
    return x_train, y_train, x_test, y_test, class_names

# Step 3: Build the Model with Transfer Learning
def build_model():
    # Load MobileNetV2 with pretrained ImageNet weights, excluding the top layer
    base_model = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=(32, 32, 3))
    base_model.trainable = True  # Allow training of the base model

    # Freeze all layers except the last 20 for fine-tuning
    for layer in base_model.layers[:-20]:
        layer.trainable = False

    # Build the full model with additional classification layers
    model = models.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),  # Pooling layer to reduce the feature map size
        layers.Dense(512, activation='relu'),  # Fully connected layer with ReLU activation
        layers.Dropout(0.5),  # Dropout layer for regularization
        layers.Dense(10, activation='softmax')  # Output layer with softmax for 10 classes
    ])

    # Compile the model with Adam optimizer and a low learning rate
    model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),
                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# Step 4: Data Augmentation
def create_data_generator(x_train):
    # Define an image data generator for augmentation
    datagen = ImageDataGenerator(
        rotation_range=20,  # Rotate images by up to 20 degrees
        width_shift_range=0.2,  # Shift images horizontally by up to 20% of width
        height_shift_range=0.2,  # Shift images vertically by up to 20% of height
        zoom_range=0.2,  # Zoom in/out by up to 20%
        horizontal_flip=True  # Flip images horizontally
    )
    datagen.fit(x_train)  # Fit the generator to the training data
    return datagen

# Step 5: Training with Callbacks for Automation
def train_model(model, datagen, x_train, y_train, x_test, y_test):
    # Set up checkpoint path to save the best model
    checkpoint_path = os.path.join(output_dir, 'best_model.keras')  # Updated to .keras extension

    # Define callbacks: reduce learning rate on plateau and save the best model
    callbacks = [
        ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=3, verbose=1, min_lr=1e-6),
        ModelCheckpoint(checkpoint_path, monitor='val_accuracy', save_best_only=True, verbose=1)
    ]

    # Train the model using data augmentation
    history = model.fit(
        datagen.flow(x_train, y_train, batch_size=32),  # Generate augmented training data
        steps_per_epoch=1800,  # Number of steps per epoch
        epochs=20,  # Number of training epochs
        validation_data=(x_test, y_test),  # Use test set for validation
        callbacks=callbacks  # Apply callbacks during training
    )
    return history, checkpoint_path

# Step 6: Evaluate the Model
def evaluate_model(model, x_test, y_test):
    # Evaluate model on the test set and print validation accuracy
    val_loss, val_acc = model.evaluate(x_test, y_test, verbose=2)
    print(f'Validation accuracy: {val_acc}')
    return val_loss, val_acc

# Step 7: Generate Predictions
def generate_predictions(model, x_test):
    # Generate predictions on the test set
    y_pred = model.predict(x_test)
    y_pred_classes = y_pred.argmax(axis=1)  # Convert probabilities to class labels
    return y_pred_classes

# Step 8: Confusion Matrix and Classification Report
def plot_confusion_matrix(y_test, y_pred_classes, class_names):
    # Generate confusion matrix to evaluate classification accuracy
    conf_matrix = confusion_matrix(y_test, y_pred_classes)
    plt.figure(figsize=(10, 8))
    # Plot confusion matrix heatmap
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()
    # Print classification report with precision, recall, and F1-score
    print(classification_report(y_test, y_pred_classes, target_names=class_names))

# Step 9: Plot Training History
def plot_training_history(history):
    # Plot training and validation accuracy
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.title('Training and Validation Accuracy')

    # Plot training and validation loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Training and Validation Loss')

    plt.show()

# Step 10: Full Pipeline Execution
def run_pipeline():
    # Load data
    x_train, y_train, x_test, y_test, class_names = load_data()

    # Build model
    model = build_model()

    # Create data generator for data augmentation
    datagen = create_data_generator(x_train)

    # Train model and save the best model to checkpoint path
    history, checkpoint_path = train_model(model, datagen, x_train, y_train, x_test, y_test)

    # Evaluate model on the test set
    evaluate_model(model, x_test, y_test)

    # Load the best model and generate predictions
    model.load_weights(checkpoint_path)
    y_pred_classes = generate_predictions(model, x_test)

    # Plot confusion matrix and classification report
    plot_confusion_matrix(y_test, y_pred_classes, class_names)

    # Plot training history for performance visualization
    plot_training_history(history)

# Run the full pipeline
run_pipeline()